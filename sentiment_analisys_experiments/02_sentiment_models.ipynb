{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca774ec",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 3 — Experiments: Financial News Sentiment Models\n",
    "\n",
    "This notebook implements the full experimental pipeline for **Section 3.7–3.10** of the thesis:\n",
    "\n",
    "- Baselines and domain models: **BERT-base** vs **FinBERT** (+ optional extra finance checkpoints)\n",
    "- Unified metrics: **Macro-F1**, **Balanced Accuracy**, **Brier score**, **Confusion Matrix**\n",
    "- **Probability calibration** (temperature scaling) + reliability diagram\n",
    "- **Error analysis** (length buckets, entity/ticker presence, hard categories)\n",
    "- Output: tables (CSV) and figures (PNG) under `./tables` and `./figures`\n",
    "\n",
    "> **Datasets expected**: `./data/news_AAPL.csv` and `./data/news_XOM.csv` with columns at minimum:  \n",
    "> `date` (YYYY-MM-DD), `headline` (str).  \n",
    "> Optionally: `label` in {negative, neutral, positive} for evaluation. If `label` is absent, the notebook will run inference and skip metric computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de91d2",
   "metadata": {},
   "source": [
    "## 0. Environment (Colab-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81644add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in Colab, uncomment the next lines:\n",
    "# !pip install -q transformers==4.43.3 datasets==2.20.0 accelerate==0.33.0 #               torch --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q scikit-learn==1.3.2 matplotlib==3.8.4 pandas==2.2.2 seaborn==0.13.2\n",
    "# !pip install -q tqdm==4.66.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781e258",
   "metadata": {},
   "source": [
    "## 1. Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a78a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, math, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, brier_score_loss\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('./data')\n",
    "FIG_DIR  = Path('./figures')\n",
    "TAB_DIR  = Path('./tables')\n",
    "OUT_DIR  = Path('./outputs')\n",
    "for d in [FIG_DIR, TAB_DIR, OUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Label mapping\n",
    "CLASSES = ['negative', 'neutral', 'positive']\n",
    "LABEL2ID = {c:i for i,c in enumerate(CLASSES)}\n",
    "ID2LABEL = {i:c for c,i in LABEL2ID.items()}\n",
    "\n",
    "def has_labels(df: pd.DataFrame) -> bool:\n",
    "    return 'label' in df.columns and df['label'].notna().sum() > 0\n",
    "\n",
    "print(\"Environment ready. CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd6306",
   "metadata": {},
   "source": [
    "## 2. Load & Prepare Data (AAPL/XOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239623c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(paths: List[Path]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if not p.exists():\n",
    "            print(f\"[WARN] Missing file: {p}. Please provide it.\")\n",
    "            continue\n",
    "        df = pd.read_csv(p)\n",
    "        frames.append(df)\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(\"No dataset files found. Please ensure CSVs are placed under ./data.\")\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "    # Standardize columns\n",
    "    if 'date' in df_all.columns:\n",
    "        df_all['date'] = pd.to_datetime(df_all['date']).dt.date.astype(str)\n",
    "    if 'headline' not in df_all.columns:\n",
    "        raise ValueError(\"Required column 'headline' is missing.\")\n",
    "    # Optional label normalization\n",
    "    if 'label' in df_all.columns:\n",
    "        df_all['label'] = df_all['label'].str.lower().str.strip()\n",
    "        df_all = df_all[df_all['label'].isin(CLASSES) | df_all['label'].isna()]\n",
    "    # Meta: ticker\n",
    "    if 'ticker' not in df_all.columns:\n",
    "        def infer_ticker(fp):\n",
    "            name = fp.name.lower()\n",
    "            if 'aapl' in name: return 'AAPL'\n",
    "            if 'xom' in name:  return 'XOM'\n",
    "            if 'cvx' in name:  return 'CVX'\n",
    "            return 'UNK'\n",
    "        sources = []\n",
    "        for p in paths:\n",
    "            if p.exists():\n",
    "                tmp = pd.read_csv(p)\n",
    "                n = len(tmp)\n",
    "                sources += [infer_ticker(p)] * n\n",
    "        if len(sources) == len(df_all):\n",
    "            df_all['ticker'] = sources\n",
    "        else:\n",
    "            df_all['ticker'] = 'UNK'\n",
    "    return df_all\n",
    "\n",
    "news_paths = [DATA_DIR/'news_AAPL.csv', DATA_DIR/'news_XOM.csv']\n",
    "df = load_dataset(news_paths)\n",
    "print(\"Loaded records:\", len(df))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fbcb3",
   "metadata": {},
   "source": [
    "## 3. Models to Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core models\n",
    "MODEL_SPECS = [\n",
    "    {\"name\":\"bert-base-uncased\", \"id\":\"bert-base-uncased\"},\n",
    "    {\"name\":\"finbert-prosus\",    \"id\":\"ProsusAI/finbert\"},\n",
    "]\n",
    "# Optional extra finance model (uncomment to add a third)\n",
    "# MODEL_SPECS.append({\"name\":\"finbert-tone\", \"id\":\"yiyanghkust/finbert-tone\"})\n",
    "print(\"Configured models:\", [m['name'] for m in MODEL_SPECS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cd6b7",
   "metadata": {},
   "source": [
    "## 4. Inference Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pipeline(model_id: str, device: Optional[str]=None) -> TextClassificationPipeline:\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    if device is None:\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "    pipe = TextClassificationPipeline(\n",
    "        model=mdl, tokenizer=tok, device=device, top_k=None, return_all_scores=True, truncation=True, max_length=64\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "def run_inference(pipe: TextClassificationPipeline, texts: List[str], batch_size: int=32) -> np.ndarray:\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        out = pipe(batch)\n",
    "        for row in out:\n",
    "            vec = np.zeros(len(CLASSES), dtype=np.float32)\n",
    "            for d in row:\n",
    "                lab = d['label'].lower()\n",
    "                if 'neg' in lab: idx = LABEL2ID['negative']\n",
    "                elif 'pos' in lab: idx = LABEL2ID['positive']\n",
    "                else: idx = LABEL2ID['neutral']\n",
    "                vec[idx] = float(d['score'])\n",
    "            s = vec.sum()\n",
    "            vec = np.array([1/3,1/3,1/3], dtype=np.float32) if s<=0 else vec/s\n",
    "            results.append(vec)\n",
    "    return np.vstack(results)\n",
    "\n",
    "def probs_to_labels(probs: np.ndarray) -> np.ndarray:\n",
    "    return probs.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080462c4",
   "metadata": {},
   "source": [
    "## 5. Metrics & Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "def brier_multiclass(y_true, probs):\n",
    "    y_true_ovr = np.eye(len(CLASSES))[y_true]\n",
    "    return np.mean(np.sum((probs - y_true_ovr)**2, axis=1))\n",
    "\n",
    "def temperature_scale(probs: np.ndarray, y_val: np.ndarray, max_iter:int=1000, lr:float=0.1) -> float:\n",
    "    eps = 1e-12\n",
    "    logits = np.log(np.clip(probs, eps, 1.0))\n",
    "    T = 1.0\n",
    "    for _ in range(max_iter):\n",
    "        scaled = logits / T\n",
    "        e = np.exp(scaled - scaled.max(axis=1, keepdims=True))\n",
    "        p = e / e.sum(axis=1, keepdims=True)\n",
    "        y_oh = np.eye(probs.shape[1])[y_val]\n",
    "        grad = np.sum((p - y_oh) * (-scaled / T), axis=None) / len(y_val)\n",
    "        T_new = T - lr * grad\n",
    "        if abs(T_new - T) < 1e-5:\n",
    "            T = T_new\n",
    "            break\n",
    "        T = max(0.05, min(5.0, T_new))\n",
    "    return float(T)\n",
    "\n",
    "def apply_temperature(probs: np.ndarray, T: float) -> np.ndarray:\n",
    "    eps = 1e-12\n",
    "    logits = np.log(np.clip(probs, eps, 1.0))\n",
    "    scaled = logits / T\n",
    "    e = np.exp(scaled - scaled.max(axis=1, keepdims=True))\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "def reliability_plot(y_true: np.ndarray, probs: np.ndarray, title:str, path:Path):\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    for i, c in enumerate(CLASSES):\n",
    "        CalibrationDisplay.from_predictions((y_true==i).astype(int), probs[:,i], n_bins=10, ax=ax, name=c)\n",
    "    ax.set_title(title); ax.grid(True); fig.tight_layout()\n",
    "    fig.savefig(path, dpi=200); plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dda9b",
   "metadata": {},
   "source": [
    "## 6. Train/Validation Split (for Calibration) & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if has_labels(df):\n",
    "    df_labeled = df.dropna(subset=['label']).copy()\n",
    "    y_all = df_labeled['label'].map(LABEL2ID).values\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(df_labeled)), test_size=0.2, random_state=SEED, stratify=y_all)\n",
    "    df_train = df_labeled.iloc[train_idx].reset_index(drop=True)\n",
    "    df_val   = df_labeled.iloc[val_idx].reset_index(drop=True)\n",
    "    print(f\"Labeled records: {len(df_labeled)} | Train: {len(df_train)} | Val: {len(df_val)}\")\n",
    "else:\n",
    "    df_train, df_val = None, None\n",
    "    print(\"No labels present — inference-only mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a274a1",
   "metadata": {},
   "source": [
    "## 7. Main Loop: Inference, Metrics, Calibration, Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c73964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = []\n",
    "\n",
    "for spec in MODEL_SPECS:\n",
    "    name, mid = spec['name'], spec['id']\n",
    "    print(f\"\\n=== Model: {name} ({mid}) ===\")\n",
    "    pipe = build_pipeline(mid)\n",
    "\n",
    "    probs_all = run_inference(pipe, df['headline'].tolist(), batch_size=32)\n",
    "    preds_all = probs_to_labels(probs_all)\n",
    "\n",
    "    out = df[['date','ticker','headline']].copy()\n",
    "    for i,c in enumerate(CLASSES):\n",
    "        out[f'proba_{c}'] = probs_all[:,i]\n",
    "    out['pred'] = [ID2LABEL[i] for i in preds_all]\n",
    "    out_path = OUT_DIR / f'{name}_inference.csv'\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"[Saved] {out_path}\")\n",
    "\n",
    "    metrics_row = {\"model\": name}\n",
    "    if df_train is not None:\n",
    "        mask_all = df['label'].notna()\n",
    "        y_true = df.loc[mask_all, 'label'].map(LABEL2ID).values\n",
    "        probs_eval = probs_all[mask_all.values]\n",
    "        y_pred = probs_eval.argmax(axis=1)\n",
    "\n",
    "        m_f1 = macro_f1(y_true, y_pred)\n",
    "        m_brier = brier_multiclass(y_true, probs_eval)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "\n",
    "        metrics_row.update({\"macro_f1_raw\": m_f1, \"brier_raw\": m_brier})\n",
    "\n",
    "        rel_path = FIG_DIR / f'{name}_reliability_raw.png'\n",
    "        reliability_plot(y_true, probs_eval, f'Reliability (raw) — {name}', rel_path)\n",
    "\n",
    "        probs_train = run_inference(pipe, df_train['headline'].tolist(), batch_size=32)\n",
    "        y_tr = df_train['label'].map(LABEL2ID).values\n",
    "        probs_val  = run_inference(pipe, df_val['headline'].tolist(), batch_size=32)\n",
    "        y_vl = df_val['label'].map(LABEL2ID).values\n",
    "\n",
    "        T = temperature_scale(probs_val, y_vl, max_iter=800, lr=0.05)\n",
    "        probs_cal = apply_temperature(probs_eval, T)\n",
    "        y_pred_cal = probs_cal.argmax(axis=1)\n",
    "\n",
    "        m_f1_cal = macro_f1(y_true, y_pred_cal)\n",
    "        m_brier_cal = brier_multiclass(y_true, probs_cal)\n",
    "\n",
    "        metrics_row.update({\"T\": T, \"macro_f1_cal\": m_f1_cal, \"brier_cal\": m_brier_cal})\n",
    "\n",
    "        # Confusion matrix fig\n",
    "        fig, ax = plt.subplots(figsize=(4.8,4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=CLASSES, yticklabels=CLASSES, ax=ax)\n",
    "        ax.set_title(f'Confusion Matrix — {name} (raw)')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(FIG_DIR/f'{name}_confusion.png', dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Calibrated reliability\n",
    "        relc_path = FIG_DIR / f'{name}_reliability_calibrated.png'\n",
    "        reliability_plot(y_true, probs_cal, f'Reliability (calibrated) — {name}', relc_path)\n",
    "\n",
    "        # Error analysis\n",
    "        df_eval = df.loc[mask_all].copy()\n",
    "        df_eval['y_true'] = y_true\n",
    "        df_eval['y_pred'] = y_pred\n",
    "        df_eval['correct'] = (df_eval['y_true'] == df_eval['y_pred']).astype(int)\n",
    "        df_eval['len'] = df_eval['headline'].astype(str).str.len()\n",
    "        df_eval['has_ticker'] = df_eval['headline'].str.contains(r'\\b(AAPL|XOM|CVX|Apple|Exxon|Chevron)\\b', case=False, regex=True)\n",
    "\n",
    "        bins = [0, 40, 80, 120, 1000]\n",
    "        labels = ['<=40','41-80','81-120','>120']\n",
    "        df_eval['len_bucket'] = pd.cut(df_eval['len'], bins=bins, labels=labels, include_lowest=True)\n",
    "        acc_by_len = df_eval.groupby('len_bucket')['correct'].mean().reset_index()\n",
    "        acc_by_len.to_csv(TAB_DIR/f'{name}_acc_by_length.csv', index=False)\n",
    "\n",
    "        err_df = df_eval[df_eval['correct']==0].copy()\n",
    "        err_df['true_label'] = err_df['y_true'].map(ID2LABEL)\n",
    "        err_df['pred_label'] = err_df['y_pred'].map(ID2LABEL)\n",
    "\n",
    "        def tag_category(text: str) -> str:\n",
    "            t = text.lower()\n",
    "            if any(w in t for w in ['investigation','lawsuit','fine','regulator','probe','antitrust','ban']):\n",
    "                return 'regulatory/legal'\n",
    "            if any(w in t for w in ['beats','misses','guidance','forecast','outlook','eps','revenue']):\n",
    "                return 'earnings/guidance'\n",
    "            if any(w in t for w in ['rumor','leak','reportedly','sources say']):\n",
    "                return 'rumors/speculation'\n",
    "            return 'general'\n",
    "\n",
    "        err_df['category'] = err_df['headline'].apply(tag_category)\n",
    "        err_df[['date','ticker','headline','true_label','pred_label','len','has_ticker','category']]            .to_csv(OUT_DIR/f'{name}_errors.csv', index=False)\n",
    "\n",
    "        err_cat = err_df['category'].value_counts().reset_index()\n",
    "        err_cat.columns = ['category','count']\n",
    "        err_cat.to_csv(TAB_DIR/f'{name}_error_categories.csv', index=False)\n",
    "\n",
    "    all_results.append(metrics_row)\n",
    "\n",
    "if all_results and any('macro_f1_raw' in r for r in all_results):\n",
    "    met = pd.DataFrame(all_results)\n",
    "    met.to_csv(TAB_DIR/'models_summary_metrics.csv', index=False)\n",
    "    met\n",
    "else:\n",
    "    print(\"No labeled data detected — metrics table not created. Inference CSVs were saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca445e",
   "metadata": {},
   "source": [
    "## 8. (Optional) Daily Aggregation Helpers for Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_daily(inf_csv: Path) -> pd.DataFrame:\n",
    "    dfp = pd.read_csv(inf_csv)\n",
    "    s = []\n",
    "    for i,row in dfp.iterrows():\n",
    "        pred = row['pred']\n",
    "        sign = -1 if pred=='negative' else (1 if pred=='positive' else 0)\n",
    "        strength = max(row['proba_negative'], row['proba_neutral'], row['proba_positive'])\n",
    "        s.append(sign * strength)\n",
    "    dfp['s_star'] = s\n",
    "    daily = (dfp.groupby(['date','ticker'])\n",
    "                 .agg(s_mean=('s_star','mean'),\n",
    "                      s_absmax=('s_star', lambda x: np.abs(x).max()),\n",
    "                      n_news=('s_star','size'))\n",
    "                 .reset_index())\n",
    "    return daily\n",
    "\n",
    "# Example (uncomment after inference):\n",
    "# daily_finbert = aggregate_daily(OUT_DIR/'finbert-prosus_inference.csv')\n",
    "# daily_finbert.to_csv(OUT_DIR/'finbert_daily_agg.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}