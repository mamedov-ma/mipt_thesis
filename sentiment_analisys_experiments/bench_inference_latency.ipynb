{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522547b9",
   "metadata": {},
   "source": [
    "\n",
    "# Inference Latency Benchmarks (BERT-base vs FinBERT)\n",
    "\n",
    "This notebook measures end-to-end **latency** and **throughput** for small batches (micro-batching=1) â€” relevant to low-latency pipelines.\n",
    "\n",
    "**It records:**\n",
    "- p50 / p95 / p99 latency (ms) over N runs\n",
    "- Mean throughput (samples/sec)\n",
    "- Optional: CPU vs CUDA (if available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117c0c3",
   "metadata": {},
   "source": [
    "## 0. Install (Colab-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f82c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If in Colab:\n",
    "# !pip install -q transformers==4.43.3 torch --index-url https://download.pytorch.org/whl/cu121 tqdm==4.66.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d43a2",
   "metadata": {},
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca48654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, statistics, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "MODELS = [\n",
    "    (\"bert-base-uncased\",\"bert-base-uncased\"),\n",
    "    (\"finbert-prosus\",\"ProsusAI/finbert\")\n",
    "]\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "N_WARMUP = 10\n",
    "N_RUNS = 200  # increase to 1000 for more stable tails\n",
    "TEXTS = [\n",
    "    \"Apple beats earnings expectations and raises guidance for Q1.\",\n",
    "    \"Regulators open antitrust investigation into major oil company operations.\",\n",
    "    \"Rumors suggest upcoming product delay; analysts remain cautious.\",\n",
    "    \"Company misses revenue estimates; shares fall after hours.\",\n",
    "    \"Chevron to acquire smaller competitor, deal valued at $10B.\"\n",
    "]\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c4a48",
   "metadata": {},
   "source": [
    "## 2. Benchmark Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pipeline(model_id: str, device: int):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, device=device, top_k=None, return_all_scores=True, truncation=True, max_length=64)\n",
    "    return pipe\n",
    "\n",
    "def sample_texts(n: int) -> list:\n",
    "    import random\n",
    "    arr = []\n",
    "    for _ in range(n):\n",
    "        arr.append(random.choice(TEXTS))\n",
    "    return arr\n",
    "\n",
    "def benchmark(pipe, n_runs=200, warmup=10):\n",
    "    # Warmup\n",
    "    _ = pipe(sample_texts(warmup))\n",
    "    times = []\n",
    "    for i in range(n_runs):\n",
    "        s = sample_texts(1)\n",
    "        t0 = time.perf_counter()\n",
    "        _ = pipe(s)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append((t1 - t0) * 1000.0)  # ms\n",
    "    import numpy as np\n",
    "    p50 = np.percentile(times, 50)\n",
    "    p95 = np.percentile(times, 95)\n",
    "    p99 = np.percentile(times, 99)\n",
    "    mean = float(np.mean(times))\n",
    "    thr = 1000.0 / mean  # samples/sec in single-sample mode\n",
    "    return {\"p50_ms\":p50, \"p95_ms\":p95, \"p99_ms\":p99, \"mean_ms\":mean, \"throughput_sps\":thr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a941a9",
   "metadata": {},
   "source": [
    "## 3. Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c073b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for name, mid in MODELS:\n",
    "    print(f\"\\n=== {name} ({mid}) ===\")\n",
    "    pipe = build_pipeline(mid, DEVICE)\n",
    "    m = benchmark(pipe, n_runs=N_RUNS, warmup=N_WARMUP)\n",
    "    m[\"model\"] = name\n",
    "    m[\"backend\"] = \"CUDA\" if DEVICE >= 0 else \"CPU\"\n",
    "    rows.append(m)\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec4b23",
   "metadata": {},
   "source": [
    "## 4. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052427a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "TAB_DIR = Path(\"./tables\"); TAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(TAB_DIR/\"bench_latency.csv\", index=False)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}