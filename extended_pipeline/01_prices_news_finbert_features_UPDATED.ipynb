{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0017a6e",
   "metadata": {},
   "source": [
    "# Notebook 1 — Prices (AAPL, XOM) + News → FinBERT → Daily sentiment index `I_t` + Features\n",
    "\n",
    "Что делает ноутбук:\n",
    "1) скачивает дневные OHLCV за последние **5 лет** (можно поменять `YEARS_BACK`);  \n",
    "2) скачивает новости (по умолчанию **Alpha Vantage News & Sentiment**; нужен API key; есть fallback на GDELT, но у него ограничение по истории);  \n",
    "3) прогоняет тексты новостей через **FinBERT (ProsusAI/finbert)**, считает `s(x)` и дневной индекс `I_t`;  \n",
    "4) считает `returns`, `RSI`, `MACD`;  \n",
    "5) собирает финальную таблицу `date, returns, RSI, MACD, I_t` (и сохраняет в файлы).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89107fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayil/Desktop/mipt_master_thesis/extended_pipeline/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies look installed.\n"
     ]
    }
   ],
   "source": [
    "# Если нужно — установите зависимости\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "required = [\n",
    "    \"pandas>=2.0\", \"numpy>=1.24\", \"requests>=2.31\", \"tqdm>=4.66\",\n",
    "    \"yfinance>=0.2.30\",\n",
    "    \"transformers>=4.40\", \"torch\",  # torch может быть уже установлен\n",
    "    \"pyarrow>=14.0\",  # для parquet\n",
    "    \"feedparser>=6.0\",  # RSS (опционально)\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for pkg in [\"pandas\",\"numpy\",\"requests\",\"tqdm\",\"yfinance\",\"transformers\",\"torch\",\"pyarrow\"]:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(\"Installing missing:\", missing)\n",
    "    pip_install(required)\n",
    "else:\n",
    "    print(\"All dependencies look installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa4498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2020-12-29 → 2025-12-29\n"
     ]
    }
   ],
   "source": [
    "import os, math, time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TICKERS = [\"AAPL\", \"XOM\"]\n",
    "YEARS_BACK = 5  # поменяйте на 3..5 по задаче\n",
    "\n",
    "END_DATE = pd.Timestamp.utcnow().normalize()\n",
    "START_DATE = END_DATE - pd.DateOffset(years=YEARS_BACK)\n",
    "\n",
    "print(\"Date range:\", START_DATE.date(), \"→\", END_DATE.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb637cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet engine (auto): pyarrow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) выберем движок\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    _PARQUET_ENGINE = \"fastparquet\"\n",
    "except Exception:\n",
    "    _PARQUET_ENGINE = \"pyarrow\"\n",
    "\n",
    "def safe_to_parquet_auto(df: pd.DataFrame, path):\n",
    "    \"\"\"\n",
    "    Универсальная запись parquet:\n",
    "    - работает независимо от того, какая версия safe_to_parquet у вас уже определена\n",
    "    - использует fastparquet если есть, иначе pyarrow\n",
    "    \"\"\"\n",
    "    # если у вас уже есть safe_to_parquet (из предыдущих ячеек) — попробуем его\n",
    "    if \"safe_to_parquet\" in globals() and callable(globals()[\"safe_to_parquet\"]):\n",
    "        fn = globals()[\"safe_to_parquet\"]\n",
    "        try:\n",
    "            # версия без engine\n",
    "            return fn(df, path)\n",
    "        except TypeError:\n",
    "            # версия с engine\n",
    "            return fn(df, path, engine=_PARQUET_ENGINE)\n",
    "\n",
    "    # fallback: прямой to_parquet\n",
    "    df.to_parquet(path, index=False, engine=_PARQUET_ENGINE)\n",
    "\n",
    "print(\"Parquet engine (auto):\", _PARQUET_ENGINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecdfc9",
   "metadata": {},
   "source": [
    "## 1) Данные по ценам (daily OHLCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet engine: pyarrow (fastparquet not found)\n",
      "AAPL (1256, 8) 2020-12-29 2025-12-29 -> prices_AAPL.parquet\n",
      "XOM (1256, 8) 2020-12-29 2025-12-29 -> prices_XOM.parquet\n",
      "Saved: data/prices_all.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapl</th>\n",
       "      <th></th>\n",
       "      <th>xom</th>\n",
       "      <th>xom</th>\n",
       "      <th>xom</th>\n",
       "      <th>xom</th>\n",
       "      <th>xom</th>\n",
       "      <th>xom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>138.050003</td>\n",
       "      <td>138.789993</td>\n",
       "      <td>134.339996</td>\n",
       "      <td>134.869995</td>\n",
       "      <td>131.289490</td>\n",
       "      <td>121047300.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>135.580002</td>\n",
       "      <td>135.990005</td>\n",
       "      <td>133.399994</td>\n",
       "      <td>133.720001</td>\n",
       "      <td>130.170029</td>\n",
       "      <td>96452100.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>134.080002</td>\n",
       "      <td>134.740005</td>\n",
       "      <td>131.720001</td>\n",
       "      <td>132.690002</td>\n",
       "      <td>129.167389</td>\n",
       "      <td>99116600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>133.610001</td>\n",
       "      <td>126.760002</td>\n",
       "      <td>129.410004</td>\n",
       "      <td>125.974472</td>\n",
       "      <td>143301900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>128.889999</td>\n",
       "      <td>131.740005</td>\n",
       "      <td>128.429993</td>\n",
       "      <td>131.009995</td>\n",
       "      <td>127.532013</td>\n",
       "      <td>97664900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price        date        open        high         low       close   adj close  \\\n",
       "Ticker                   aapl        aapl        aapl        aapl        aapl   \n",
       "0      2020-12-29  138.050003  138.789993  134.339996  134.869995  131.289490   \n",
       "1      2020-12-30  135.580002  135.990005  133.399994  133.720001  130.170029   \n",
       "2      2020-12-31  134.080002  134.740005  131.720001  132.690002  129.167389   \n",
       "3      2021-01-04  133.520004  133.610001  126.760002  129.410004  125.974472   \n",
       "4      2021-01-05  128.889999  131.740005  128.429993  131.009995  127.532013   \n",
       "\n",
       "Price        volume ticker open high low close adj close volume  \n",
       "Ticker         aapl         xom  xom xom   xom       xom    xom  \n",
       "0       121047300.0   AAPL  NaN  NaN NaN   NaN       NaN    NaN  \n",
       "1        96452100.0   AAPL  NaN  NaN NaN   NaN       NaN    NaN  \n",
       "2        99116600.0   AAPL  NaN  NaN NaN   NaN       NaN    NaN  \n",
       "3       143301900.0   AAPL  NaN  NaN NaN   NaN       NaN    NaN  \n",
       "4        97664900.0   AAPL  NaN  NaN NaN   NaN       NaN    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# --- 0) DATA_DIR (на всякий случай) ---\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1) Выбор engine: fastparquet -> предпочтительно ---\n",
    "_PARQUET_ENGINE = None\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    _PARQUET_ENGINE = \"fastparquet\"\n",
    "    print(\"Parquet engine:\", _PARQUET_ENGINE)\n",
    "except Exception:\n",
    "    _PARQUET_ENGINE = \"pyarrow\"\n",
    "    print(\"Parquet engine:\", _PARQUET_ENGINE, \"(fastparquet not found)\")\n",
    "\n",
    "# --- 2) Safe Parquet writer ---\n",
    "def _reset_pyarrow_pandas_ext_types():\n",
    "    # безопасно: если pyarrow нет/другая версия — просто пропустим\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "    # точечные самые частые\n",
    "    for name in (\"pandas.period\", \"pandas.interval\"):\n",
    "        try:\n",
    "            pa.unregister_extension_type(name)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # попытка подчистить всё pandas.* если API доступен\n",
    "    for attr in (\"registered_extension_types\", \"get_registered_extension_types\", \"list_registered_extension_types\"):\n",
    "        try:\n",
    "            fn = getattr(pa, attr)\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            reg = fn()\n",
    "            # pyarrow может вернуть dict или list\n",
    "            if isinstance(reg, dict):\n",
    "                names = list(reg.keys())\n",
    "            else:\n",
    "                names = list(reg)\n",
    "            for n in names:\n",
    "                n = str(n)\n",
    "                if n.startswith(\"pandas.\"):\n",
    "                    try:\n",
    "                        pa.unregister_extension_type(n)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def safe_to_parquet(df: pd.DataFrame, path, engine: str):\n",
    "    \"\"\"\n",
    "    Пишем Parquet устойчиво.\n",
    "    - Если ловим ArrowKeyError про pandas.* already defined — чистим registry и повторяем 1 раз.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_parquet(path, index=False, engine=engine)\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if (\"type extension with name pandas.\" in msg and \"already defined\" in msg) or \"ArrowKeyError\" in msg:\n",
    "            _reset_pyarrow_pandas_ext_types()\n",
    "            df.to_parquet(path, index=False, engine=engine)  # retry once\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def download_prices(ticker: str, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    df = yf.download(\n",
    "        ticker, start=start.date(), end=(end + pd.Timedelta(days=1)).date(),\n",
    "        interval=\"1d\", auto_adjust=False, progress=False\n",
    "    )\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"No price data for {ticker}\")\n",
    "    df = df.rename(columns=str.lower)\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    df = df.reset_index().rename(columns={\"Date\":\"date\", \"index\":\"date\"})\n",
    "    df[\"ticker\"] = ticker\n",
    "    return df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj close\",\"volume\",\"ticker\"]]\n",
    "\n",
    "# --- 3) Скачивание и сохранение ---\n",
    "prices_list = []\n",
    "\n",
    "for t in TICKERS:\n",
    "    dft = download_prices(t, START_DATE, END_DATE)  # ваша функция из предыдущей ячейки\n",
    "    prices_list.append(dft)\n",
    "\n",
    "    out_path = DATA_DIR / f\"prices_{t}.parquet\"\n",
    "    safe_to_parquet(dft, out_path, engine=_PARQUET_ENGINE)\n",
    "\n",
    "    print(\n",
    "        t,\n",
    "        dft.shape,\n",
    "        dft[\"date\"].min().date(),\n",
    "        dft[\"date\"].max().date(),\n",
    "        \"->\",\n",
    "        out_path.name,\n",
    "    )\n",
    "\n",
    "prices = pd.concat(prices_list, ignore_index=True)\n",
    "\n",
    "all_path = DATA_DIR / \"prices_all.parquet\"\n",
    "safe_to_parquet(prices, all_path, engine=_PARQUET_ENGINE)\n",
    "print(\"Saved:\", all_path)\n",
    "\n",
    "prices.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cedeb40",
   "metadata": {},
   "source": [
    "## 1b) Intraday цены (1h) для event-study (Yahoo / yfinance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6d014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intraday: prices_intraday_1h_AAPL.parquet (8274, 8) 2024-01-02 09:00:00 2025-12-29 14:30:00\n",
      "Saved intraday: prices_intraday_1h_XOM.parquet (8298, 8) 2024-01-02 09:00:00 2025-12-29 14:30:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02 09:00:00</td>\n",
       "      <td>191.68</td>\n",
       "      <td>191.75</td>\n",
       "      <td>189.75</td>\n",
       "      <td>190.07</td>\n",
       "      <td>190.07</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02 10:00:00</td>\n",
       "      <td>190.07</td>\n",
       "      <td>190.25</td>\n",
       "      <td>189.80</td>\n",
       "      <td>190.06</td>\n",
       "      <td>190.06</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-02 11:00:00</td>\n",
       "      <td>190.08</td>\n",
       "      <td>190.20</td>\n",
       "      <td>188.87</td>\n",
       "      <td>188.87</td>\n",
       "      <td>188.87</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-02 12:00:00</td>\n",
       "      <td>188.92</td>\n",
       "      <td>189.65</td>\n",
       "      <td>188.79</td>\n",
       "      <td>189.19</td>\n",
       "      <td>189.19</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 13:00:00</td>\n",
       "      <td>189.18</td>\n",
       "      <td>192.20</td>\n",
       "      <td>187.78</td>\n",
       "      <td>187.99</td>\n",
       "      <td>187.99</td>\n",
       "      <td>0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            datetime    open    high     low   close  adj_close  volume  \\\n",
       "0     2024-01-02 09:00:00  191.68  191.75  189.75  190.07     190.07       0   \n",
       "1     2024-01-02 10:00:00  190.07  190.25  189.80  190.06     190.06       0   \n",
       "2     2024-01-02 11:00:00  190.08  190.20  188.87  188.87     188.87       0   \n",
       "3     2024-01-02 12:00:00  188.92  189.65  188.79  189.19     189.19       0   \n",
       "4     2024-01-02 13:00:00  189.18  192.20  187.78  187.99     187.99       0   \n",
       "\n",
       "Price ticker  \n",
       "0       AAPL  \n",
       "1       AAPL  \n",
       "2       AAPL  \n",
       "3       AAPL  \n",
       "4       AAPL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "def _to_utc_naive(ts) -> pd.Timestamp:\n",
    "    \"\"\"Convert anything timestamp-like to UTC-naive pandas Timestamp.\"\"\"\n",
    "    ts = pd.Timestamp(ts)\n",
    "    if ts.tzinfo is not None:\n",
    "        ts = ts.tz_convert(\"UTC\").tz_localize(None)\n",
    "    return ts\n",
    "\n",
    "def download_prices_intraday_1h_yahoo(ticker: str, start, end, period_days: int = 729) -> pd.DataFrame:\n",
    "    start = _to_utc_naive(start)\n",
    "    end = _to_utc_naive(end)\n",
    "\n",
    "    df = yf.download(\n",
    "        ticker,\n",
    "        interval=\"1h\",\n",
    "        period=f\"{period_days}d\",\n",
    "        auto_adjust=False,\n",
    "        progress=False,\n",
    "        prepost=True,\n",
    "    )\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"No intraday price data for {ticker} via Yahoo (period={period_days}d)\")\n",
    "\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "    df = df.rename(columns=lambda c: str(c).strip().lower().replace(\" \", \"_\"))\n",
    "\n",
    "    idx = pd.to_datetime(df.index)\n",
    "    # make UTC-aware then UTC-naive\n",
    "    if getattr(idx, \"tz\", None) is None:\n",
    "        idx = idx.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        idx = idx.tz_convert(\"UTC\")\n",
    "    idx = idx.tz_localize(None)\n",
    "\n",
    "    df = df.reset_index().rename(columns={\"Datetime\": \"datetime\", \"Date\": \"datetime\", \"index\": \"datetime\"})\n",
    "    df[\"datetime\"] = idx\n",
    "    df[\"ticker\"] = ticker\n",
    "\n",
    "    if \"adj_close\" not in df.columns:\n",
    "        df[\"adj_close\"] = df.get(\"close\", np.nan)\n",
    "\n",
    "    out = df[[\"datetime\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\", \"ticker\"]].copy()\n",
    "    out[\"volume\"] = pd.to_numeric(out[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "    # ✅ clip safely (all UTC-naive)\n",
    "    end_inclusive = end + pd.Timedelta(days=1) - pd.Timedelta(nanoseconds=1)\n",
    "    out = out[(out[\"datetime\"] >= start) & (out[\"datetime\"] <= end_inclusive)].copy()\n",
    "    out = out.sort_values(\"datetime\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----- use it -----\n",
    "INTRADAY_PERIOD_DAYS = 729\n",
    "\n",
    "intr_end = _to_utc_naive(END_DATE)\n",
    "intr_start = _to_utc_naive(max(pd.Timestamp(START_DATE), pd.Timestamp(END_DATE) - pd.Timedelta(days=INTRADAY_PERIOD_DAYS)))\n",
    "\n",
    "intraday_prices = []\n",
    "for t in TICKERS:\n",
    "    out_path = DATA_DIR / f\"prices_intraday_1h_{t}.parquet\"\n",
    "    if out_path.exists():\n",
    "        dfi = safe_read_parquet(out_path)\n",
    "        # на всякий: нормализуем если в кеше вдруг tz-aware\n",
    "        dfi[\"datetime\"] = pd.to_datetime(dfi[\"datetime\"]).dt.tz_localize(None)\n",
    "        print(\"Loaded cached intraday:\", out_path.name, dfi.shape, dfi[\"datetime\"].min(), dfi[\"datetime\"].max())\n",
    "    else:\n",
    "        dfi = download_prices_intraday_1h_yahoo(t, intr_start, intr_end, period_days=INTRADAY_PERIOD_DAYS)\n",
    "        if dfi.empty:\n",
    "            print(f\"WARNING: {t} intraday empty after clipping to [{intr_start}..{intr_end}]\")\n",
    "        else:\n",
    "            safe_to_parquet_auto(dfi, out_path)\n",
    "            print(\"Saved intraday:\", out_path.name, dfi.shape, dfi[\"datetime\"].min(), dfi[\"datetime\"].max())\n",
    "\n",
    "    intraday_prices.append(dfi)\n",
    "\n",
    "intraday_prices = pd.concat(intraday_prices, ignore_index=True)\n",
    "safe_to_parquet_auto(intraday_prices, DATA_DIR / \"prices_intraday_1h_all.parquet\")\n",
    "intraday_prices.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae347649",
   "metadata": {},
   "source": [
    "## 2) Новости + маппинг по датам\n",
    "\n",
    "### Источник новостей\n",
    "- **Основной (рекомендуется): Alpha Vantage `NEWS_SENTIMENT`** — поддерживает `time_from/time_to` (можно брать 3–5 лет), но нужен API key. citeturn3view0  \n",
    "- **Fallback: GDELT DOC API** — без ключа, но по сути ограничен коротким окном истории (не подойдет для 3–5 лет). citeturn1view0\n",
    "\n",
    "В коде ниже:\n",
    "- если `ALPHAVANTAGE_API_KEY` задан, используем Alpha Vantage;\n",
    "- иначе используем GDELT на коротком промежутке (чтобы ноутбук всё равно работал)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45866962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_ALPHA_VANTAGE = True\n"
     ]
    }
   ],
   "source": [
    "# TB228JYIOYDWU5Q9\n",
    "\n",
    "ALPHAVANTAGE_API_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"\").strip()\n",
    "ALPHAVANTAGE_API_KEY = \"TB228JYIOYDWU5Q9\"\n",
    "USE_ALPHA_VANTAGE = bool(ALPHAVANTAGE_API_KEY)\n",
    "\n",
    "\n",
    "print(\"USE_ALPHA_VANTAGE =\", USE_ALPHA_VANTAGE)\n",
    "if not USE_ALPHA_VANTAGE:\n",
    "    print(\"⚠️  Нет ALPHAVANTAGE_API_KEY. Будет fallback на GDELT/RSS (история GDELT обычно ограничена последними ~3 месяцами).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e96363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yyyymmddThhmm(ts: pd.Timestamp) -> str:\n",
    "    # Alpha Vantage: YYYYMMDDTHHMM (UTC)\n",
    "    ts = ts.tz_localize(timezone.utc) if ts.tzinfo is None else ts.tz_convert(timezone.utc)\n",
    "    return ts.strftime(\"%Y%m%dT%H%M\")\n",
    "\n",
    "def alpha_vantage_news_window(ticker: str, time_from: pd.Timestamp, time_to: pd.Timestamp, limit: int = 1000, sort: str=\"EARLIEST\") -> pd.DataFrame:\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"tickers\": ticker,\n",
    "        \"time_from\": yyyymmddThhmm(time_from),\n",
    "        \"time_to\": yyyymmddThhmm(time_to),\n",
    "        \"limit\": limit,\n",
    "        \"sort\": sort,\n",
    "        \"apikey\": ALPHAVANTAGE_API_KEY,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    # возможные ошибки: {'Information': '...'} / {'Note': '...'}\n",
    "    if \"feed\" not in data:\n",
    "        raise RuntimeError(f\"Alpha Vantage response without 'feed': {list(data.keys())}\")\n",
    "\n",
    "    rows = []\n",
    "    for it in data[\"feed\"]:\n",
    "        rows.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"time_published\": it.get(\"time_published\"),\n",
    "            \"title\": it.get(\"title\"),\n",
    "            \"summary\": it.get(\"summary\"),\n",
    "            \"url\": it.get(\"url\"),\n",
    "            \"source\": it.get(\"source\"),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def fetch_alpha_vantage_news(ticker: str, start: pd.Timestamp, end: pd.Timestamp, window_days: int = 30) -> pd.DataFrame:\n",
    "    # chunk по окнам, чтобы не упираться в limit и проще переживать rate-limit\n",
    "    all_parts = []\n",
    "    cur = start\n",
    "    pbar = tqdm(total=int((end-start).days/window_days)+1, desc=f\"AV news {ticker}\")\n",
    "    while cur < end:\n",
    "        nxt = min(cur + pd.Timedelta(days=window_days), end)\n",
    "        try:\n",
    "            part = alpha_vantage_news_window(ticker, cur, nxt, limit=1000, sort=\"EARLIEST\")\n",
    "            all_parts.append(part)\n",
    "        except Exception as e:\n",
    "            print(\"Window failed:\", cur.date(), \"→\", nxt.date(), \":\", repr(e))\n",
    "        # free-tier rate limit обычно 5 запросов/мин → пауза; при необходимости уменьшите\n",
    "        time.sleep(12)\n",
    "        cur = nxt\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    if not all_parts:\n",
    "        return pd.DataFrame(columns=[\"ticker\",\"time_published\",\"title\",\"summary\",\"url\",\"source\"])\n",
    "\n",
    "    df = pd.concat(all_parts, ignore_index=True).drop_duplicates(subset=[\"url\"])\n",
    "    return df\n",
    "\n",
    "# GDELT fallback (короткое окно истории)\n",
    "def fetch_gdelt_artlist(query: str, timespan: str = \"3m\", maxrecords: int = 250) -> pd.DataFrame:\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"artlist\",\n",
    "        \"format\": \"json\",\n",
    "        \"timespan\": timespan,\n",
    "        \"maxrecords\": maxrecords,\n",
    "        \"sort\": \"datedesc\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    arts = data.get(\"articles\", [])\n",
    "    rows = []\n",
    "    for a in arts:\n",
    "        rows.append({\n",
    "            \"time_published\": a.get(\"seendate\"),\n",
    "            \"title\": a.get(\"title\"),\n",
    "            \"summary\": a.get(\"snippet\"),\n",
    "            \"url\": a.get(\"url\"),\n",
    "            \"source\": a.get(\"sourceCountry\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528dbe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AV news AAPL: 100%|██████████| 61/61 [13:12<00:00, 12.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: news_raw_AAPL.parquet (3602, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AV news XOM: 100%|██████████| 61/61 [13:08<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: news_raw_XOM.parquet (2754, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Intel shares rise after Third Point urges chip...</td>\n",
       "      <td>https://www.cnbc.com/2020/12/29/third-point-ur...</td>\n",
       "      <td>2020-12-29 12:56:00</td>\n",
       "      <td>Intel shares rise after Third Point urges chip...</td>\n",
       "      <td>2020-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Exclusive: Hedge fund Third Point urges Intel ...</td>\n",
       "      <td>https://www.reuters.com/business/retail-consum...</td>\n",
       "      <td>2020-12-30 02:24:00</td>\n",
       "      <td>Exclusive: Hedge fund Third Point urges Intel ...</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Apple loses copyright claims in lawsuit agains...</td>\n",
       "      <td>https://www.reuters.com/business/apple-loses-c...</td>\n",
       "      <td>2020-12-30 05:28:00</td>\n",
       "      <td>Apple loses copyright claims in lawsuit agains...</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Apple Veterans’ Lidar Startup Adds $200 Millio...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2021-0...</td>\n",
       "      <td>2021-01-04 12:00:00</td>\n",
       "      <td>Apple Veterans’ Lidar Startup Adds $200 Millio...</td>\n",
       "      <td>2021-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>The TDVG ETF Is a Stellar Choice for Dividend ...</td>\n",
       "      <td>https://etfdb.com/active-etf-channel/tdvg-etf-...</td>\n",
       "      <td>2021-01-04 21:22:02</td>\n",
       "      <td>The TDVG ETF Is a Stellar Choice for Dividend ...</td>\n",
       "      <td>2021-01-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker         source                                              title  \\\n",
       "0   AAPL  alpha_vantage  Intel shares rise after Third Point urges chip...   \n",
       "1   AAPL  alpha_vantage  Exclusive: Hedge fund Third Point urges Intel ...   \n",
       "2   AAPL  alpha_vantage  Apple loses copyright claims in lawsuit agains...   \n",
       "3   AAPL  alpha_vantage  Apple Veterans’ Lidar Startup Adds $200 Millio...   \n",
       "4   AAPL  alpha_vantage  The TDVG ETF Is a Stellar Choice for Dividend ...   \n",
       "\n",
       "                                                 url        published_at  \\\n",
       "0  https://www.cnbc.com/2020/12/29/third-point-ur... 2020-12-29 12:56:00   \n",
       "1  https://www.reuters.com/business/retail-consum... 2020-12-30 02:24:00   \n",
       "2  https://www.reuters.com/business/apple-loses-c... 2020-12-30 05:28:00   \n",
       "3  https://www.bloomberg.com/news/articles/2021-0... 2021-01-04 12:00:00   \n",
       "4  https://etfdb.com/active-etf-channel/tdvg-etf-... 2021-01-04 21:22:02   \n",
       "\n",
       "                                                text       date  \n",
       "0  Intel shares rise after Third Point urges chip... 2020-12-29  \n",
       "1  Exclusive: Hedge fund Third Point urges Intel ... 2020-12-30  \n",
       "2  Apple loses copyright claims in lawsuit agains... 2020-12-30  \n",
       "3  Apple Veterans’ Lidar Startup Adds $200 Millio... 2021-01-04  \n",
       "4  The TDVG ETF Is a Stellar Choice for Dividend ... 2021-01-04  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Новости: Alpha Vantage (если есть ключ) + GDELT (последние ~3 месяца) + RSS (последние недели) ---\n",
    "# Результат: единый news DataFrame с полями:\n",
    "# ticker, source, title, url, published_at (UTC-naive), date (UTC day, UTC-naive), text\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import feedparser\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- helpers: unify everything to UTC-naive ----\n",
    "def to_datetime_utc_naive(x) -> pd.Timestamp:\n",
    "    \"\"\"Parse datetime-like and return UTC-naive pandas Timestamp (or NaT).\"\"\"\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)) or (isinstance(x, str) and not x.strip()):\n",
    "        return pd.NaT\n",
    "\n",
    "    # already a datetime/timestamp\n",
    "    if isinstance(x, (pd.Timestamp, datetime)):\n",
    "        ts = pd.Timestamp(x)\n",
    "        if ts.tzinfo is not None:\n",
    "            ts = ts.tz_convert(\"UTC\").tz_localize(None)\n",
    "        return ts\n",
    "\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Alpha Vantage style: YYYYMMDDTHHMMSS / YYYYMMDDTHHMM\n",
    "    try:\n",
    "        if \"T\" in s and len(s) >= 13 and s[:8].isdigit():\n",
    "            if len(s) >= 15:\n",
    "                ts = pd.to_datetime(s[:15], format=\"%Y%m%dT%H%M%S\", utc=True, errors=\"coerce\")\n",
    "            else:\n",
    "                ts = pd.to_datetime(s[:13], format=\"%Y%m%dT%H%M\", utc=True, errors=\"coerce\")\n",
    "        else:\n",
    "            ts = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        ts = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "\n",
    "    if ts is pd.NaT or pd.isna(ts):\n",
    "        return pd.NaT\n",
    "    return ts.tz_convert(\"UTC\").tz_localize(None)\n",
    "\n",
    "def to_utc_naive_day(x) -> pd.Timestamp:\n",
    "    \"\"\"Convert anything timestamp-like to UTC-naive day (00:00).\"\"\"\n",
    "    ts = pd.Timestamp(x)\n",
    "    if ts.tzinfo is not None:\n",
    "        ts = ts.tz_convert(\"UTC\").tz_localize(None)\n",
    "    return ts.normalize()\n",
    "\n",
    "# normalize START/END once here (fixes tz-aware vs tz-naive comparisons everywhere below)\n",
    "START_DAY = to_utc_naive_day(START_DATE)\n",
    "END_DAY   = to_utc_naive_day(END_DATE)\n",
    "\n",
    "def add_date_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"published_at\"] = df[\"published_at\"].apply(to_datetime_utc_naive)\n",
    "    # ensure dtype and UTC-naive\n",
    "    df[\"published_at\"] = pd.to_datetime(df[\"published_at\"]).dt.tz_localize(None)\n",
    "    df[\"date\"] = df[\"published_at\"].dt.floor(\"D\").dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "# ---- sources ----\n",
    "def fetch_google_news_rss(query: str, max_items: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    RSS history is short (weeks/months). Helps to increase news density recently.\n",
    "    \"\"\"\n",
    "    url = \"https://news.google.com/rss/search\"\n",
    "    params = {\"q\": query, \"hl\": \"en-US\", \"gl\": \"US\", \"ceid\": \"US:en\"}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    feed = feedparser.parse(r.text)\n",
    "\n",
    "    rows = []\n",
    "    for e in feed.entries[:max_items]:\n",
    "        rows.append({\n",
    "            \"title\": getattr(e, \"title\", None),\n",
    "            \"url\": getattr(e, \"link\", None),\n",
    "            \"published_at\": getattr(e, \"published\", None) or getattr(e, \"updated\", None),\n",
    "            \"source\": \"google_news_rss\",\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def fetch_gdelt_artlist(query: str, timespan: str = \"3m\", maxrecords: int = 250, sort: str = \"datedesc\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    GDELT DOC 2.0 API. timespan is relative (e.g., 3m).\n",
    "    \"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"artlist\",\n",
    "        \"format\": \"json\",\n",
    "        \"maxrecords\": maxrecords,\n",
    "        \"sort\": sort,\n",
    "        \"timespan\": timespan,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    arts = js.get(\"articles\", []) or []\n",
    "\n",
    "    rows = []\n",
    "    for a in arts:\n",
    "        rows.append({\n",
    "            \"title\": a.get(\"title\"),\n",
    "            \"url\": a.get(\"url\"),\n",
    "            \"published_at\": a.get(\"seendate\") or a.get(\"date\") or a.get(\"datetime\"),\n",
    "            \"source\": \"gdelt\",\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---- normalizers ----\n",
    "def normalize_alpha_vantage(df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"ticker\",\"source\",\"title\",\"url\",\"published_at\",\"text\",\"date\"])\n",
    "    out = df.copy()\n",
    "    out[\"ticker\"] = ticker\n",
    "    out[\"source\"] = \"alpha_vantage\"\n",
    "    out[\"published_at\"] = out.get(\"time_published\")\n",
    "    summary = out.get(\"summary\", out.get(\"description\", \"\"))\n",
    "    out[\"text\"] = (out.get(\"title\", \"\").astype(str) + \". \" + summary.astype(str)).str.strip()\n",
    "    out = out[[\"ticker\",\"source\",\"title\",\"url\",\"published_at\",\"text\"]]\n",
    "    out = add_date_cols(out)\n",
    "    return out\n",
    "\n",
    "def normalize_generic(df: pd.DataFrame, ticker: str, source_default: str) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"ticker\",\"source\",\"title\",\"url\",\"published_at\",\"text\",\"date\"])\n",
    "    out = df.copy()\n",
    "    out[\"ticker\"] = ticker\n",
    "    out[\"source\"] = out.get(\"source\", source_default)\n",
    "    out[\"published_at\"] = out.get(\"published_at\")\n",
    "    out[\"title\"] = out.get(\"title\")\n",
    "    out[\"url\"] = out.get(\"url\")\n",
    "    out[\"text\"] = out.get(\"text\", out.get(\"title\", \"\")).astype(str)\n",
    "    out = out[[\"ticker\",\"source\",\"title\",\"url\",\"published_at\",\"text\"]]\n",
    "    out = add_date_cols(out)\n",
    "    return out\n",
    "\n",
    "# ---- build dataset ----\n",
    "news_all = []\n",
    "\n",
    "for t in TICKERS:\n",
    "    out_path = DATA_DIR / f\"news_raw_{t}.parquet\"\n",
    "\n",
    "    if out_path.exists():\n",
    "        df = safe_read_parquet(out_path)\n",
    "        # ensure date dtype is UTC-naive for safe comparisons later\n",
    "        df[\"published_at\"] = pd.to_datetime(df[\"published_at\"]).dt.tz_localize(None)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "        print(\"Loaded cached:\", out_path.name, df.shape)\n",
    "    else:\n",
    "        parts = []\n",
    "\n",
    "        if USE_ALPHA_VANTAGE:\n",
    "            df_av = fetch_alpha_vantage_news(t, START_DATE, END_DATE, window_days=30)\n",
    "            parts.append(normalize_alpha_vantage(df_av, t))\n",
    "        else:\n",
    "            df_g = fetch_gdelt_artlist(query=f\"{t} stock\", timespan=\"3m\", maxrecords=250)\n",
    "            parts.append(normalize_generic(df_g, t, \"gdelt\"))\n",
    "\n",
    "        # RSS (short history) — best effort\n",
    "        try:\n",
    "            df_rss = fetch_google_news_rss(f\"{t} stock\", max_items=200)\n",
    "            parts.append(normalize_generic(df_rss, t, \"google_news_rss\"))\n",
    "        except Exception as e:\n",
    "            print(f\"RSS skipped for {t}: {e}\")\n",
    "\n",
    "        df = pd.concat([p for p in parts if p is not None and not p.empty], ignore_index=True)\n",
    "\n",
    "        # basic cleanup + safe types\n",
    "        df[\"published_at\"] = pd.to_datetime(df[\"published_at\"]).dt.tz_localize(None)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "        df = df.dropna(subset=[\"published_at\", \"title\"]).copy()\n",
    "        df = df.drop_duplicates(subset=[\"url\", \"title\", \"published_at\"]).copy()\n",
    "\n",
    "        # ✅ tz-safe filter by day range\n",
    "        df = df[(df[\"date\"] >= START_DAY) & (df[\"date\"] <= END_DAY)].copy()\n",
    "\n",
    "        safe_to_parquet_auto(df, out_path)\n",
    "        print(\"Saved:\", out_path.name, df.shape)\n",
    "\n",
    "    news_all.append(df)\n",
    "\n",
    "news_all = (\n",
    "    pd.concat(news_all, ignore_index=True)\n",
    "      .sort_values([\"ticker\", \"published_at\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "safe_to_parquet_auto(news_all, DATA_DIR / \"news_raw_all.parquet\")\n",
    "news_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874d347",
   "metadata": {},
   "source": [
    "## 3) FinBERT → `s(x)` и дневной индекс `I_t`\n",
    "\n",
    "Модель: **ProsusAI/finbert** (3 класса: positive/negative/neutral). citeturn0search1  \n",
    "\n",
    "Считаем для каждой новости:\n",
    "\n",
    "- `p_pos, p_neg, p_neu` — softmax вероятности\n",
    "- `s(x) = p_pos - p_neg` (в диапазоне [-1, 1]) — удобная непрерывная метрика citeturn0search13\n",
    "\n",
    "Далее агрегируем по дню и тикеру:\n",
    "\n",
    "- `I_t = mean(s(x))` по всем новостям в этот день."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa55282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "labels: {0: 'positive', 1: 'negative', 2: 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"device:\", device)\n",
    "print(\"labels:\", model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e2da302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News rows: 6355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Intel shares rise after Third Point urges chip...</td>\n",
       "      <td>https://www.cnbc.com/2020/12/29/third-point-ur...</td>\n",
       "      <td>2020-12-29 12:56:00</td>\n",
       "      <td>Intel shares rise after Third Point urges chip...</td>\n",
       "      <td>2020-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Apple loses copyright claims in lawsuit agains...</td>\n",
       "      <td>https://www.reuters.com/business/apple-loses-c...</td>\n",
       "      <td>2020-12-30 05:28:00</td>\n",
       "      <td>Apple loses copyright claims in lawsuit agains...</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Exclusive: Hedge fund Third Point urges Intel ...</td>\n",
       "      <td>https://www.reuters.com/business/retail-consum...</td>\n",
       "      <td>2020-12-30 02:24:00</td>\n",
       "      <td>Exclusive: Hedge fund Third Point urges Intel ...</td>\n",
       "      <td>2020-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>The TDVG ETF Is a Stellar Choice for Dividend ...</td>\n",
       "      <td>https://etfdb.com/active-etf-channel/tdvg-etf-...</td>\n",
       "      <td>2021-01-04 21:22:02</td>\n",
       "      <td>The TDVG ETF Is a Stellar Choice for Dividend ...</td>\n",
       "      <td>2021-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Apple Veterans’ Lidar Startup Adds $200 Millio...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2021-0...</td>\n",
       "      <td>2021-01-04 12:00:00</td>\n",
       "      <td>Apple Veterans’ Lidar Startup Adds $200 Millio...</td>\n",
       "      <td>2021-01-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker         source                                              title  \\\n",
       "0   AAPL  alpha_vantage  Intel shares rise after Third Point urges chip...   \n",
       "1   AAPL  alpha_vantage  Apple loses copyright claims in lawsuit agains...   \n",
       "2   AAPL  alpha_vantage  Exclusive: Hedge fund Third Point urges Intel ...   \n",
       "3   AAPL  alpha_vantage  The TDVG ETF Is a Stellar Choice for Dividend ...   \n",
       "4   AAPL  alpha_vantage  Apple Veterans’ Lidar Startup Adds $200 Millio...   \n",
       "\n",
       "                                                 url        published_at  \\\n",
       "0  https://www.cnbc.com/2020/12/29/third-point-ur... 2020-12-29 12:56:00   \n",
       "1  https://www.reuters.com/business/apple-loses-c... 2020-12-30 05:28:00   \n",
       "2  https://www.reuters.com/business/retail-consum... 2020-12-30 02:24:00   \n",
       "3  https://etfdb.com/active-etf-channel/tdvg-etf-... 2021-01-04 21:22:02   \n",
       "4  https://www.bloomberg.com/news/articles/2021-0... 2021-01-04 12:00:00   \n",
       "\n",
       "                                                text       date  \n",
       "0  Intel shares rise after Third Point urges chip... 2020-12-29  \n",
       "1  Apple loses copyright claims in lawsuit agains... 2020-12-30  \n",
       "2  Exclusive: Hedge fund Third Point urges Intel ... 2020-12-30  \n",
       "3  The TDVG ETF Is a Stellar Choice for Dividend ... 2021-01-04  \n",
       "4  Apple Veterans’ Lidar Startup Adds $200 Millio... 2021-01-04  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finbert_scores(texts, batch_size: int = 16, max_length: int = 256):\n",
    "    # returns np.array shape [n,3] aligned with model.config.id2label\n",
    "    probs_all = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"FinBERT\", leave=False):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, truncation=True, padding=True, max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "    return np.vstack(probs_all) if probs_all else np.zeros((0,3), dtype=float)\n",
    "\n",
    "def build_text(row) -> str:\n",
    "    title = row.get(\"title\") or \"\"\n",
    "    summary = row.get(\"summary\") or \"\"\n",
    "    txt = (title + \". \" + summary).strip()\n",
    "    return txt[:5000]  # safety cap\n",
    "\n",
    "news = news_all.copy()\n",
    "news[\"text\"] = news.apply(build_text, axis=1)\n",
    "\n",
    "# ограничим пустые/короткие тексты\n",
    "news = news[news[\"text\"].str.len() >= 5].copy()\n",
    "news = news.sort_values([\"ticker\",\"date\",\"url\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"News rows:\", len(news))\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c1198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet engine: pyarrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/news_scored_all.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>p_pos</th>\n",
       "      <th>p_neg</th>\n",
       "      <th>p_neu</th>\n",
       "      <th>s_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Intel shares rise after Third Point urges chip...</td>\n",
       "      <td>https://www.cnbc.com/2020/12/29/third-point-ur...</td>\n",
       "      <td>2020-12-29 12:56:00</td>\n",
       "      <td>Intel shares rise after Third Point urges chip...</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>0.214336</td>\n",
       "      <td>0.741410</td>\n",
       "      <td>0.044254</td>\n",
       "      <td>-0.527073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Apple loses copyright claims in lawsuit agains...</td>\n",
       "      <td>https://www.reuters.com/business/apple-loses-c...</td>\n",
       "      <td>2020-12-30 05:28:00</td>\n",
       "      <td>Apple loses copyright claims in lawsuit agains...</td>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.906648</td>\n",
       "      <td>0.065647</td>\n",
       "      <td>-0.878942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Exclusive: Hedge fund Third Point urges Intel ...</td>\n",
       "      <td>https://www.reuters.com/business/retail-consum...</td>\n",
       "      <td>2020-12-30 02:24:00</td>\n",
       "      <td>Exclusive: Hedge fund Third Point urges Intel ...</td>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>0.649670</td>\n",
       "      <td>0.015036</td>\n",
       "      <td>0.335294</td>\n",
       "      <td>0.634633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>The TDVG ETF Is a Stellar Choice for Dividend ...</td>\n",
       "      <td>https://etfdb.com/active-etf-channel/tdvg-etf-...</td>\n",
       "      <td>2021-01-04 21:22:02</td>\n",
       "      <td>The TDVG ETF Is a Stellar Choice for Dividend ...</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>0.902477</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.088037</td>\n",
       "      <td>0.892991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>alpha_vantage</td>\n",
       "      <td>Apple Veterans’ Lidar Startup Adds $200 Millio...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2021-0...</td>\n",
       "      <td>2021-01-04 12:00:00</td>\n",
       "      <td>Apple Veterans’ Lidar Startup Adds $200 Millio...</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>0.438414</td>\n",
       "      <td>0.012534</td>\n",
       "      <td>0.549052</td>\n",
       "      <td>0.425879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker         source                                              title  \\\n",
       "0   AAPL  alpha_vantage  Intel shares rise after Third Point urges chip...   \n",
       "1   AAPL  alpha_vantage  Apple loses copyright claims in lawsuit agains...   \n",
       "2   AAPL  alpha_vantage  Exclusive: Hedge fund Third Point urges Intel ...   \n",
       "3   AAPL  alpha_vantage  The TDVG ETF Is a Stellar Choice for Dividend ...   \n",
       "4   AAPL  alpha_vantage  Apple Veterans’ Lidar Startup Adds $200 Millio...   \n",
       "\n",
       "                                                 url        published_at  \\\n",
       "0  https://www.cnbc.com/2020/12/29/third-point-ur... 2020-12-29 12:56:00   \n",
       "1  https://www.reuters.com/business/apple-loses-c... 2020-12-30 05:28:00   \n",
       "2  https://www.reuters.com/business/retail-consum... 2020-12-30 02:24:00   \n",
       "3  https://etfdb.com/active-etf-channel/tdvg-etf-... 2021-01-04 21:22:02   \n",
       "4  https://www.bloomberg.com/news/articles/2021-0... 2021-01-04 12:00:00   \n",
       "\n",
       "                                                text       date     p_pos  \\\n",
       "0  Intel shares rise after Third Point urges chip... 2020-12-29  0.214336   \n",
       "1  Apple loses copyright claims in lawsuit agains... 2020-12-30  0.027705   \n",
       "2  Exclusive: Hedge fund Third Point urges Intel ... 2020-12-30  0.649670   \n",
       "3  The TDVG ETF Is a Stellar Choice for Dividend ... 2021-01-04  0.902477   \n",
       "4  Apple Veterans’ Lidar Startup Adds $200 Millio... 2021-01-04  0.438414   \n",
       "\n",
       "      p_neg     p_neu       s_x  \n",
       "0  0.741410  0.044254 -0.527073  \n",
       "1  0.906648  0.065647 -0.878942  \n",
       "2  0.015036  0.335294  0.634633  \n",
       "3  0.009486  0.088037  0.892991  \n",
       "4  0.012534  0.549052  0.425879  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1) Parquet safe writer (самодостаточно) ---\n",
    "_PARQUET_ENGINE = None\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    _PARQUET_ENGINE = \"fastparquet\"\n",
    "except Exception:\n",
    "    _PARQUET_ENGINE = \"pyarrow\"\n",
    "\n",
    "def _reset_pyarrow_pandas_ext_types():\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "    except Exception:\n",
    "        return\n",
    "    for name in (\"pandas.period\", \"pandas.interval\"):\n",
    "        try:\n",
    "            pa.unregister_extension_type(name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    for attr in (\"registered_extension_types\", \"get_registered_extension_types\", \"list_registered_extension_types\"):\n",
    "        try:\n",
    "            fn = getattr(pa, attr)\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            reg = fn()\n",
    "            names = list(reg.keys()) if isinstance(reg, dict) else list(reg)\n",
    "            for n in names:\n",
    "                n = str(n)\n",
    "                if n.startswith(\"pandas.\"):\n",
    "                    try:\n",
    "                        pa.unregister_extension_type(n)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def safe_to_parquet(df: pd.DataFrame, path):\n",
    "    try:\n",
    "        df.to_parquet(path, index=False, engine=_PARQUET_ENGINE)\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if (\"type extension with name pandas.\" in msg and \"already defined\" in msg) or \"ArrowKeyError\" in msg:\n",
    "            _reset_pyarrow_pandas_ext_types()\n",
    "            df.to_parquet(path, index=False, engine=_PARQUET_ENGINE)  # retry once\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"Parquet engine:\", _PARQUET_ENGINE)\n",
    "\n",
    "# --- 2) FinBERT label mapping (robust к разным вариантам id2label) ---\n",
    "id2label = {int(k): str(v).lower() for k, v in model.config.id2label.items()}\n",
    "\n",
    "pos_id = next((i for i, lab in id2label.items() if \"pos\" in lab), None)\n",
    "neg_id = next((i for i, lab in id2label.items() if \"neg\" in lab), None)\n",
    "neu_id = next((i for i, lab in id2label.items() if \"neu\" in lab), None)\n",
    "\n",
    "if pos_id is None or neg_id is None:\n",
    "    raise RuntimeError(f\"Unexpected FinBERT labels: {id2label}\")\n",
    "\n",
    "# --- 3) Готовим тексты: гарантируем str и без NaN ---\n",
    "if \"text\" not in news.columns:\n",
    "    raise RuntimeError(\"news must have a 'text' column\")\n",
    "\n",
    "texts = news[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# --- 4) Скоринг ---\n",
    "probs = finbert_scores(texts, batch_size=16, max_length=256)\n",
    "\n",
    "# probs может быть list -> в ndarray\n",
    "probs = np.asarray(probs, dtype=\"float32\")\n",
    "\n",
    "news[\"p_pos\"] = probs[:, pos_id]\n",
    "news[\"p_neg\"] = probs[:, neg_id]\n",
    "news[\"p_neu\"] = probs[:, neu_id] if neu_id is not None else np.nan\n",
    "news[\"s_x\"] = news[\"p_pos\"] - news[\"p_neg\"]\n",
    "\n",
    "# --- 5) Сохранение ---\n",
    "out_path = DATA_DIR / \"news_scored_all.parquet\"\n",
    "safe_to_parquet(news, out_path)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb2afe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78902/1536664558.py:33: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  news[\"hour\"] = news[\"published_at\"].dt.floor(\"H\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  ticker       date       I_t   I_t_max  N_t_strong  n_news   I_t_pos  \\\n",
       " 0   AAPL 2020-12-29 -0.527073  0.527073         1.0       1       NaN   \n",
       " 1   AAPL 2020-12-30 -0.122154  0.878942         2.0       2  0.634633   \n",
       " 2   AAPL 2021-01-04  0.659435  0.892991         1.0       2  0.659435   \n",
       " 3   AAPL 2021-01-05 -0.162034  0.510277         1.0       3  0.060650   \n",
       " 4   AAPL 2021-01-06 -0.073685  0.262288         0.0       3  0.075242   \n",
       " \n",
       "     I_t_neg  \n",
       " 0 -0.527073  \n",
       " 1 -0.878942  \n",
       " 2       NaN  \n",
       " 3 -0.273376  \n",
       " 4 -0.148149  ,\n",
       "   ticker            datetime       I_h   I_h_max  N_h_strong  n_news\n",
       " 0   AAPL 2020-12-29 12:00:00 -0.527073  0.527073         1.0       1\n",
       " 1   AAPL 2020-12-30 02:00:00  0.634633  0.634633         1.0       1\n",
       " 2   AAPL 2020-12-30 05:00:00 -0.878942  0.878942         1.0       1\n",
       " 3   AAPL 2021-01-04 12:00:00  0.425879  0.425879         0.0       1\n",
       " 4   AAPL 2021-01-04 21:00:00  0.892991  0.892991         1.0       1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Дневные и часовые индексы новостного фона ---\n",
    "# Требуется: news (после скоринга, с колонками: ticker, published_at, date, s_x)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TAU_STRONG = 0.5  # порог \"сильной\" новости по |s(x)| (можете менять)\n",
    "\n",
    "def _mean_pos(x):\n",
    "    x = x[x > 0]\n",
    "    return x.mean() if len(x) else np.nan\n",
    "\n",
    "def _mean_neg(x):\n",
    "    x = x[x < 0]\n",
    "    return x.mean() if len(x) else np.nan\n",
    "\n",
    "# 1) Daily\n",
    "daily_idx = (\n",
    "    news.groupby([\"ticker\",\"date\"], as_index=False)\n",
    "        .agg(\n",
    "            I_t=(\"s_x\",\"mean\"),\n",
    "            I_t_max=(\"s_x\", lambda v: np.nanmax(np.abs(v.values)) if len(v) else np.nan),\n",
    "            N_t_strong=(\"s_x\", lambda v: float(np.sum(np.abs(v.values) >= TAU_STRONG))),\n",
    "            n_news=(\"s_x\",\"size\"),\n",
    "            I_t_pos=(\"s_x\", _mean_pos),\n",
    "            I_t_neg=(\"s_x\", _mean_neg),\n",
    "        )\n",
    ")\n",
    "\n",
    "safe_to_parquet(daily_idx, DATA_DIR / \"daily_sentiment_indices.parquet\")\n",
    "\n",
    "# 2) Hourly (1h bins, UTC)\n",
    "news[\"hour\"] = news[\"published_at\"].dt.floor(\"H\")\n",
    "hourly_idx = (\n",
    "    news.dropna(subset=[\"hour\"])\n",
    "        .groupby([\"ticker\",\"hour\"], as_index=False)\n",
    "        .agg(\n",
    "            I_h=(\"s_x\",\"mean\"),\n",
    "            I_h_max=(\"s_x\", lambda v: np.nanmax(np.abs(v.values)) if len(v) else np.nan),\n",
    "            N_h_strong=(\"s_x\", lambda v: float(np.sum(np.abs(v.values) >= TAU_STRONG))),\n",
    "            n_news=(\"s_x\",\"size\"),\n",
    "        )\n",
    "        .rename(columns={\"hour\":\"datetime\"})\n",
    ")\n",
    "\n",
    "safe_to_parquet(hourly_idx, DATA_DIR / \"hourly_sentiment_indices_1h.parquet\")\n",
    "\n",
    "daily_idx.head(), hourly_idx.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f66906",
   "metadata": {},
   "source": [
    "## 4) Returns + RSI + MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8f8c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'ticker', 'open', 'high', 'low', 'close', 'adj_close',\n",
      "       'volume'],\n",
      "      dtype='object')\n",
      "        date ticker        open        high         low       close  \\\n",
      "0 2020-12-29   AAPL  138.050003  138.789993  134.339996  134.869995   \n",
      "1 2020-12-30   AAPL  135.580002  135.990005  133.399994  133.720001   \n",
      "2 2020-12-31   AAPL  134.080002  134.740005  131.720001  132.690002   \n",
      "3 2021-01-04   AAPL  133.520004  133.610001  126.760002  129.410004   \n",
      "4 2021-01-05   AAPL  128.889999  131.740005  128.429993  131.009995   \n",
      "\n",
      "    adj_close       volume  \n",
      "0  131.289490  121047300.0  \n",
      "1  130.170029   96452100.0  \n",
      "2  129.167389   99116600.0  \n",
      "3  125.974472  143301900.0  \n",
      "4  127.532013   97664900.0  \n",
      "ticker\n",
      "AAPL    1256\n",
      "XOM     1256\n",
      "Name: count, dtype: Int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78902/1502786367.py:33: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  p.stack(level=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def yf_multiindex_to_long(prices: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not isinstance(prices.columns, pd.MultiIndex):\n",
    "        raise TypeError(f\"Expected MultiIndex columns, got {type(prices.columns)}\")\n",
    "\n",
    "    p = prices.copy()\n",
    "\n",
    "    def norm(x) -> str:\n",
    "        return str(x).strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "    # 1) нормализуем имена полей (level 0) и тикеры (level 1)\n",
    "    p.columns = pd.MultiIndex.from_tuples([\n",
    "        (norm(a), str(b).strip().upper() if b is not None else \"\")\n",
    "        for a, b in p.columns\n",
    "    ])\n",
    "\n",
    "    # 2) дата: у тебя она колонкой ('date','') -> делаем индексом\n",
    "    if (\"date\", \"\") in p.columns:\n",
    "        p = p.set_index((\"date\", \"\"))\n",
    "        p.index.name = \"date\"\n",
    "    else:\n",
    "        # если даты нет колонкой, считаем что индекс и есть дата\n",
    "        p.index.name = p.index.name or \"date\"\n",
    "\n",
    "    # 3) ВАЖНО: выкидываем служебный ('ticker',''), чтобы не было конфликта при reset_index()\n",
    "    if (\"ticker\", \"\") in p.columns:\n",
    "        p = p.drop(columns=[(\"ticker\", \"\")])\n",
    "\n",
    "    # 4) stack по тикеру (level 1)\n",
    "    out = (\n",
    "        p.stack(level=1)\n",
    "         .rename_axis(index=[\"date\", \"ticker\"])\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "    # 5) финальная чистка\n",
    "    out.columns = [norm(c) for c in out.columns]\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"ticker\"] = out[\"ticker\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "    # оставим основные поля, если есть\n",
    "    keep = [c for c in [\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"] if c in out.columns]\n",
    "    out = (\n",
    "        out[keep]\n",
    "        .dropna(subset=[\"date\",\"ticker\"])\n",
    "        .sort_values([\"ticker\",\"date\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# --- usage ---\n",
    "prices = yf_multiindex_to_long(prices)\n",
    "\n",
    "# тикеры в том же виде, что в prices\n",
    "TICKERS = [str(t).strip().upper() for t in TICKERS]\n",
    "\n",
    "print(prices.columns)\n",
    "print(prices.head())\n",
    "print(prices[\"ticker\"].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f8ac94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>returns</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>MACD_hist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>131.289490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>130.170029</td>\n",
       "      <td>-0.008527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.089302</td>\n",
       "      <td>-0.017860</td>\n",
       "      <td>-0.071441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>129.167389</td>\n",
       "      <td>-0.007703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.238232</td>\n",
       "      <td>-0.061935</td>\n",
       "      <td>-0.176298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>125.974472</td>\n",
       "      <td>-0.024719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.606907</td>\n",
       "      <td>-0.170929</td>\n",
       "      <td>-0.435978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>127.532013</td>\n",
       "      <td>0.012364</td>\n",
       "      <td>8.684289</td>\n",
       "      <td>-0.764590</td>\n",
       "      <td>-0.289661</td>\n",
       "      <td>-0.474928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker   adj_close   returns       RSI      MACD  MACD_signal  \\\n",
       "0 2020-12-29   AAPL  131.289490       NaN       NaN  0.000000     0.000000   \n",
       "1 2020-12-30   AAPL  130.170029 -0.008527  0.000000 -0.089302    -0.017860   \n",
       "2 2020-12-31   AAPL  129.167389 -0.007703  0.000000 -0.238232    -0.061935   \n",
       "3 2021-01-04   AAPL  125.974472 -0.024719  0.000000 -0.606907    -0.170929   \n",
       "4 2021-01-05   AAPL  127.532013  0.012364  8.684289 -0.764590    -0.289661   \n",
       "\n",
       "   MACD_hist  \n",
       "0   0.000000  \n",
       "1  -0.071441  \n",
       "2  -0.176298  \n",
       "3  -0.435978  \n",
       "4  -0.474928  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) sanity: нормализуем названия колонок (если вдруг где-то иначе) ---\n",
    "prices = prices.copy()\n",
    "prices.columns = [str(c).strip().lower().replace(\" \", \"_\") for c in prices.columns]\n",
    "\n",
    "# --- 1) выбираем правильную колонку цены для расчётов ---\n",
    "# приоритет: adj_close -> close\n",
    "price_col = \"adj_close\" if \"adj_close\" in prices.columns else (\"close\" if \"close\" in prices.columns else None)\n",
    "if price_col is None:\n",
    "    raise RuntimeError(f\"prices must contain adj_close or close. Got columns: {list(prices.columns)}\")\n",
    "\n",
    "# --- 2) индикаторы ---\n",
    "def ema(series: pd.Series, span: int) -> pd.Series:\n",
    "    return series.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = (-delta).clip(lower=0)\n",
    "    avg_gain = gain.ewm(alpha=1/period, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def macd(close: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "    macd_line = ema(close, fast) - ema(close, slow)\n",
    "    signal_line = ema(macd_line, signal)\n",
    "    hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, hist\n",
    "\n",
    "# --- 3) Фичи по каждому тикеру ---\n",
    "feat_parts = []\n",
    "\n",
    "for t in TICKERS:\n",
    "    p = prices.loc[prices[\"ticker\"] == t].copy()\n",
    "\n",
    "    if p.empty:\n",
    "        print(f\"WARNING: no rows for {t}\")\n",
    "        continue\n",
    "\n",
    "    p = p.sort_values(\"date\")\n",
    "\n",
    "    px = pd.to_numeric(p[price_col], errors=\"coerce\")\n",
    "    p[\"returns\"] = px.pct_change()  # close-to-close\n",
    "    p[\"RSI\"] = rsi(px, period=14)\n",
    "    macd_line, signal_line, hist = macd(px)\n",
    "    p[\"MACD\"] = macd_line\n",
    "    p[\"MACD_signal\"] = signal_line\n",
    "    p[\"MACD_hist\"] = hist\n",
    "\n",
    "    feat_parts.append(p)\n",
    "\n",
    "feat_prices = pd.concat(feat_parts, ignore_index=True)\n",
    "\n",
    "# (опционально) оставим только ключевые колонки + индикаторы\n",
    "keep_cols = [c for c in [\"date\", \"ticker\", price_col, \"returns\", \"RSI\", \"MACD\", \"MACD_signal\", \"MACD_hist\"] if c in feat_prices.columns]\n",
    "feat_prices_out = feat_prices[keep_cols].copy()\n",
    "\n",
    "safe_to_parquet(feat_prices_out, DATA_DIR / \"price_features.parquet\")\n",
    "feat_prices_out.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fcb8da",
   "metadata": {},
   "source": [
    "## 5) Финальная таблица `date, returns, RSI, MACD, I_t`\n",
    "\n",
    "Склеиваем price-features и `I_t` по (`ticker`, `date`). Если в какой-то день новостей нет — `I_t` будет NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7caf1983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>returns</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>I_t</th>\n",
       "      <th>I_t_max</th>\n",
       "      <th>N_t_strong</th>\n",
       "      <th>n_news</th>\n",
       "      <th>I_t_pos</th>\n",
       "      <th>I_t_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2025-12-22</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>54.443250</td>\n",
       "      <td>0.438767</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>0.906059</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.308234</td>\n",
       "      <td>-0.702637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>57.885258</td>\n",
       "      <td>0.579341</td>\n",
       "      <td>0.179612</td>\n",
       "      <td>0.901233</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.315180</td>\n",
       "      <td>-0.362660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2025-12-24</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>57.152934</td>\n",
       "      <td>0.666920</td>\n",
       "      <td>0.548885</td>\n",
       "      <td>0.927096</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.588975</td>\n",
       "      <td>-0.012382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2025-12-26</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>56.727837</td>\n",
       "      <td>0.719162</td>\n",
       "      <td>0.062667</td>\n",
       "      <td>0.832491</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>-0.304680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2025-12-29</td>\n",
       "      <td>0.009572</td>\n",
       "      <td>60.044899</td>\n",
       "      <td>0.842844</td>\n",
       "      <td>-0.134916</td>\n",
       "      <td>0.952407</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.534848</td>\n",
       "      <td>-0.358170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker       date   returns        RSI      MACD       I_t   I_t_max  \\\n",
       "2507    XOM 2025-12-22  0.012512  54.443250  0.438767  0.181875  0.906059   \n",
       "2508    XOM 2025-12-23  0.010749  57.885258  0.579341  0.179612  0.901233   \n",
       "2509    XOM 2025-12-24 -0.001675  57.152934  0.666920  0.548885  0.927096   \n",
       "2510    XOM 2025-12-26 -0.000923  56.727837  0.719162  0.062667  0.832491   \n",
       "2511    XOM 2025-12-29  0.009572  60.044899  0.842844 -0.134916  0.952407   \n",
       "\n",
       "      N_t_strong  n_news   I_t_pos   I_t_neg  \n",
       "2507         9.0    24.0  0.308234 -0.702637  \n",
       "2508         7.0    20.0  0.315180 -0.362660  \n",
       "2509         9.0    15.0  0.588975 -0.012382  \n",
       "2510         2.0    17.0  0.141384 -0.304680  \n",
       "2511         2.0     4.0  0.534848 -0.358170  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Финальная дневная таблица: returns + RSI + MACD + новостные индексы ---\n",
    "# Требуется: feat_prices (цены+фичи) и daily_idx (из предыдущей секции)\n",
    "\n",
    "try:\n",
    "    d_idx = daily_idx.copy()\n",
    "except NameError:\n",
    "    d_idx = safe_read_parquet(DATA_DIR / \"daily_sentiment_indices.parquet\")\n",
    "\n",
    "final = feat_prices.merge(\n",
    "    d_idx,\n",
    "    on=[\"ticker\",\"date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# минимальный набор (как в ТЗ)\n",
    "final_small = final[[\"ticker\",\"date\",\"returns\",\"RSI\",\"MACD\",\"I_t\"]].copy()\n",
    "safe_to_parquet(final_small, DATA_DIR / \"final_features_all.parquet\")\n",
    "\n",
    "# расширенный набор (для анализа порогов/интенсивности)\n",
    "final_ext = final[[\"ticker\",\"date\",\"returns\",\"RSI\",\"MACD\",\"I_t\",\"I_t_max\",\"N_t_strong\",\"n_news\",\"I_t_pos\",\"I_t_neg\"]].copy()\n",
    "safe_to_parquet(final_ext, DATA_DIR / \"final_features_all_extended.parquet\")\n",
    "\n",
    "# также отдельные файлы по тикерам\n",
    "for t in TICKERS:\n",
    "    safe_to_parquet(final_small[final_small[\"ticker\"] == t].copy(), DATA_DIR / f\"final_features_{t}.parquet\")\n",
    "    safe_to_parquet(final_ext[final_ext[\"ticker\"] == t].copy(), DATA_DIR / f\"final_features_extended_{t}.parquet\")\n",
    "\n",
    "final_ext.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cf16d",
   "metadata": {},
   "source": [
    "## 6) Intraday (1h) фичи + часовой новостной индекс\n",
    "\n",
    "Собираем датасет на **часовой** частоте: `datetime, ticker, r_fwd1h, r_fwd3h, I_h, I_h_max, N_h_strong, n_news`  \n",
    "(используется в Notebook 2 для intraday-корреляций и event-study)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44bcfb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>r_fwd1h</th>\n",
       "      <th>r_fwd3h</th>\n",
       "      <th>I_h</th>\n",
       "      <th>I_h_max</th>\n",
       "      <th>N_h_strong</th>\n",
       "      <th>n_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02 09:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>190.07</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.004630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02 10:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>190.06</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.006261</td>\n",
       "      <td>-0.010891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-02 11:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>188.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-02 12:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>189.19</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.006343</td>\n",
       "      <td>-0.015858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 13:00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>187.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>-0.006304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime ticker  adj_close  volume   r_fwd1h   r_fwd3h  I_h  \\\n",
       "0 2024-01-02 09:00:00   AAPL     190.07       0 -0.000053 -0.004630  NaN   \n",
       "1 2024-01-02 10:00:00   AAPL     190.06       0 -0.006261 -0.010891  NaN   \n",
       "2 2024-01-02 11:00:00   AAPL     188.87       0  0.001694 -0.003653  NaN   \n",
       "3 2024-01-02 12:00:00   AAPL     189.19       0 -0.006343 -0.015858  NaN   \n",
       "4 2024-01-02 13:00:00   AAPL     187.99       0  0.001011 -0.006304  NaN   \n",
       "\n",
       "   I_h_max  N_h_strong  n_news  \n",
       "0      NaN         NaN     NaN  \n",
       "1      NaN         NaN     NaN  \n",
       "2      NaN         NaN     NaN  \n",
       "3      NaN         NaN     NaN  \n",
       "4      NaN         NaN     NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Merge intraday prices with hourly sentiment index ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Ensure we have intraday_prices (from section 1b) and hourly_idx (from indices section)\n",
    "try:\n",
    "    intr = intraday_prices.copy()\n",
    "except NameError:\n",
    "    intr = safe_read_parquet(DATA_DIR / \"prices_intraday_1h_all.parquet\")\n",
    "\n",
    "try:\n",
    "    hidx = hourly_idx.copy()\n",
    "except NameError:\n",
    "    hidx = safe_read_parquet(DATA_DIR / \"hourly_sentiment_indices_1h.parquet\")\n",
    "\n",
    "intr = intr.sort_values([\"ticker\",\"datetime\"]).copy()\n",
    "\n",
    "# forward returns from hour t close -> hour t+1/t+3 close\n",
    "intr[\"r_fwd1h\"] = intr.groupby(\"ticker\")[\"adj_close\"].pct_change().shift(-1)\n",
    "intr[\"r_fwd3h\"] = intr.groupby(\"ticker\")[\"adj_close\"].pct_change(3).shift(-3)\n",
    "\n",
    "# merge\n",
    "intr_merged = intr.merge(hidx, on=[\"ticker\",\"datetime\"], how=\"left\")\n",
    "\n",
    "# keep useful columns\n",
    "keep_cols = [\"datetime\",\"ticker\",\"adj_close\",\"volume\",\"r_fwd1h\",\"r_fwd3h\",\"I_h\",\"I_h_max\",\"N_h_strong\",\"n_news\"]\n",
    "for col in keep_cols:\n",
    "    if col not in intr_merged.columns:\n",
    "        intr_merged[col] = np.nan\n",
    "intr_merged = intr_merged[keep_cols].sort_values([\"ticker\",\"datetime\"])\n",
    "\n",
    "safe_to_parquet(intr_merged, DATA_DIR / \"final_features_intraday_1h.parquet\")\n",
    "intr_merged.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
